{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# Материалы к проекту по идентификации пользователей\n",
    "\n",
    "<center> Исполнитель: Глазунов А.В."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые и просто полезные библиотеки и установки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 50)\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import  accuracy_score,roc_auc_score,f1_score,classification_report,roc_curve, confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "numpy 1.16.4\n",
      "scipy 1.2.1\n",
      "pandas 0.24.2\n",
      "matplotlib 3.1.0\n",
      "statsmodels 0.9.0\n",
      "sklearn 0.22.2.post1\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_set(path_to_csv_files, session_length=10):\n",
    "    \n",
    "   \n",
    "    files_paths = list(glob(os.path.join(path_to_csv_files,'user*.csv')))\n",
    "    \n",
    "    files = []\n",
    "    user_IDs = []\n",
    "    for path in tqdm_notebook(files_paths):\n",
    "        f_name = os.path.split(path)[1]\n",
    "        ID = int(re.findall(\"[0-9]+\",f_name)[0])\n",
    "        user_IDs.append(ID)\n",
    "        files.append(pd.read_csv(path))\n",
    "        \n",
    "    sites_dictionary = {}\n",
    "    \n",
    "    for user in tqdm_notebook(files):\n",
    "        for site in user.site.values:\n",
    "            if site in sites_dictionary:\n",
    "               \n",
    "                sites_dictionary[site] += 1                \n",
    "                \n",
    "            else:\n",
    "                sites_dictionary[site] = 1\n",
    "              \n",
    "    d =  sorted(sites_dictionary.items(), key=lambda item: item[1],reverse = True)    \n",
    "    sites_dict_sorted = {}\n",
    "    \n",
    "    for ii,pair in enumerate(d):\n",
    "        \n",
    "        sites_dict_sorted[pair[0]] = [ii+1,pair[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    data_list = []  \n",
    "        \n",
    "    for ind,user in enumerate(tqdm_notebook(files)): \n",
    "        session = np.zeros(session_length+1)\n",
    "        session[session_length] = user_IDs[ind]\n",
    "        site_position = 0\n",
    "        for site in user.site.values:\n",
    "            session[site_position]=sites_dict_sorted[site][0]                \n",
    "            site_position += 1\n",
    "            if site_position == session_length:\n",
    "                data_list.append(session)                  \n",
    "                session = np.zeros(session_length+1)\n",
    "                session[session_length] = user_IDs[ind]\n",
    "                site_position = 0\n",
    "        if site_position != 0:\n",
    "            data_list.append(session)\n",
    "    \n",
    "    columns = ['site'+str(num) for num in range(1,session_length+1)]+['user_ID']\n",
    "    data = pd.DataFrame(data_list,dtype = int)\n",
    "    data.columns= columns   \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return data,sites_dict_sorted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_format_doc(sessions):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for s in tqdm_notebook(sessions):\n",
    "        for ID in s:\n",
    "            index = vocabulary.setdefault(ID, ID)\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    return csr_matrix((data, indices, indptr), dtype=int)[:,1:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_format(X):\n",
    "    data = np.ones(X.size, dtype=int)\n",
    "    indices = X.reshape(-1)\n",
    "    indptr = np.arange(X.shape[0] + 1) * X.shape[1]\n",
    "    return csr_matrix((data, indices, indptr), dtype=int)[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_train_set_window(path_to_csv_files, site_freq_path, \n",
    "                                    session_length=10, window_size=10):\n",
    "        \n",
    "    #Загрузка файлов пользователей\n",
    "    files_paths = list(glob(os.path.join(path_to_csv_files,'user*.csv')))\n",
    "    files = []\n",
    "    user_IDs = []\n",
    "    for path in tqdm_notebook(files_paths):\n",
    "        f_name = os.path.split(path)[1]\n",
    "        ID = int(re.findall(\"[0-9]+\",f_name)[0])\n",
    "        user_IDs.append(ID)\n",
    "        files.append(pd.read_csv(path))\n",
    "    user_IDs = np.array(user_IDs)\n",
    "            \n",
    "    #Получение словаря сайтов с ID и частотами\n",
    "    if (session_length==10) and (window_size==10):#если так, то подгружаем уже готовый\n",
    "        with open(site_freq_path,'rb') as f:\n",
    "            sites_dict_sorted = pickle.load(f)\n",
    "        f.close()\n",
    "    else:        #иначе создаем из файлов пользователй\n",
    "        sites_dictionary = {}\n",
    "    \n",
    "        for user in tqdm_notebook(files):\n",
    "            for site in user.site.values:\n",
    "                if site in sites_dictionary:\n",
    "               \n",
    "                    sites_dictionary[site] += 1                \n",
    "                \n",
    "                else:\n",
    "                    sites_dictionary[site] = 1\n",
    "              \n",
    "        d =  sorted(sites_dictionary.items(), key=lambda item: item[1],reverse = True)    \n",
    "        sites_dict_sorted = {}\n",
    "    \n",
    "        for ii,pair in enumerate(d):\n",
    "        \n",
    "            sites_dict_sorted[pair[0]] = [ii+1,pair[1]]  #заполнение словаря ID и частотами из отсортированного d\n",
    "            \n",
    "    \n",
    "    # Заполнение таблицы пользовательских сессий ID просмотренных пользователем сайтов\n",
    "    Session_list = []  \n",
    "        \n",
    "    for ind,user in enumerate(tqdm_notebook(files)): \n",
    "        session = np.zeros(session_length+1)\n",
    "        session[session_length] = user_IDs[ind] #заполнение стобца ID пользователей\n",
    "        \n",
    "        sites = user.site.values\n",
    "        ufile_length = sites.shape[0]\n",
    "        site_position = 0 #позиция в сессии\n",
    "        pos_in_ufile = 0 #позиция в файле\n",
    "        while pos_in_ufile < ufile_length:            \n",
    "            session[site_position]=sites_dict_sorted[sites[pos_in_ufile]][0]#получение ID сайта из словаря по названию\n",
    "            pos_in_ufile += 1\n",
    "            site_position += 1\n",
    "            \n",
    "            if site_position == session_length:\n",
    "                Session_list.append(session)                  \n",
    "                session = np.zeros(session_length+1)\n",
    "                session[session_length] = user_IDs[ind]\n",
    "                site_position = 0\n",
    "                pos_in_ufile -= session_length - window_size\n",
    "           \n",
    "            if (pos_in_ufile == ufile_length) & (site_position != 0):\n",
    "                Session_list.append(session)\n",
    "                if site_position > window_size:\n",
    "                    session = np.zeros(session_length+1)\n",
    "                    session[session_length] = user_IDs[ind]               \n",
    "                    pos_in_ufile -= site_position - window_size\n",
    "                    site_position = 0  \n",
    "\n",
    "    \n",
    "    Sessions = np.array(Session_list,dtype = int)\n",
    "        \n",
    "    #Отделений ID пользователей от таблицы\n",
    "    X, y = Sessions[:,:-1],Sessions[:,-1]\n",
    "    \n",
    "    #Получение разреженной матрицы частот встречаемости сайтов в каждой сессии      \n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for s in tqdm_notebook(X):\n",
    "        for ID in s:\n",
    "            index = vocabulary.setdefault(ID, ID)\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    X_sparse = csr_matrix((data, indices, indptr), dtype=int)[:,1:] \n",
    "    \n",
    "    \n",
    "    return X_sparse, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_data_10users.csv'), \n",
    "                       index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['user_ID'].value_counts("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем распределение числа уникальных сайтов в каждой сессии из 10 посещенных подряд сайтов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_sites = [np.unique(train_df.values[i, :-1]).shape[0] \n",
    "                    for i in range(train_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(num_unique_sites).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте с помощью QQ-плота и критерия Шапиро-Уилка, что эта величина распределена нормально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(num_unique_sites, plot=plt)\n",
    "plt.show()\n",
    "print(\"Shapiro-Wilk normality test, W-statistic: %f, p-value: %f\" % stats.shapiro(num_unique_sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим гипотезу о том, что пользователь хотя бы раз зайдет на сайт, который он уже ранее посетил в сессии из 10 сайтов. Проверим с помощью биномиального критерия для доли, что доля случаев, когда пользователь повторно посетил какой-то сайт (то есть число уникальных сайтов в сессии < 10) велика: больше 95% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_two_similar = (np.array(num_unique_sites) < 10).astype('int')\n",
    "has_two_similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_val = stats.binom_test(sum(has_two_similar),has_two_similar.shape[0], p=0.95,alternative = 'greater')\n",
    "pi_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 95% доверительный интервал Уилсона для доли случаев, когда пользователь повторно посетил какой-то сайт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilson_interval = proportion_confint(sum(has_two_similar), has_two_similar.shape[0], method = 'wilson')\n",
    "wilson_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение частоты посещения сайтов (сколько раз тот или иной сайт попадается в выборке) для сайтов, которые были посещены как минимум 1000 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA,'site_freq_10users.pkl'), 'rb') as f:\n",
    "    freq_dict = pickle.load(f)\n",
    "    f.close()\n",
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freqs = pd.DataFrame(freq_dict).T\n",
    "df_freqs[df_freqs[1]>=1000][1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_freqs = df_freqs[df_freqs[1]>=1000][1].values\n",
    "site_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каков 95% доверительный интервал для средней частоты появления сайта в выборке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples, random_seed=17):\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_intervals(stat, alpha):\n",
    "    boundaries = np.percentile(stat, \n",
    "                 [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_bootstrap_samples(df_freqs[1].values,len(df_freqs[1].values))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = list(map(np.mean, samples))\n",
    "df_freqs[1].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = stat_intervals(means,0.05)[0]\n",
    "right = stat_intervals(means,0.05)[1]\n",
    "print(round(left,3),round(right,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_set_with_fe(path_to_csv_files, site_freq_path, feature_names,\n",
    "                                    session_length=10, window_size=10):\n",
    "    ''' ВАШ КОД ЗДЕСЬ '''\n",
    "    \n",
    "    \n",
    "    #Загрузка файлов пользователей\n",
    "    files_paths = list(glob(os.path.join(path_to_csv_files,'user*.csv')))\n",
    "    files = []\n",
    "    user_IDs = []\n",
    "    for path in tqdm_notebook(files_paths):\n",
    "        f_name = os.path.split(path)[1]\n",
    "        ID = int(re.findall(\"[0-9]+\",f_name)[0])\n",
    "        user_IDs.append(ID)\n",
    "        files.append(pd.read_csv(path))\n",
    "    user_IDs = np.array(user_IDs)\n",
    "            \n",
    "    #Получение словаря сайтов с ID и частотами\n",
    "    if (session_length==10) and (window_size==10):#если так, то подгружаем уже готовый\n",
    "        with open(site_freq_path,'rb') as f:\n",
    "            sites_dict_sorted = pickle.load(f)\n",
    "        f.close()\n",
    "    else:        #иначе создаем из файлов пользователй\n",
    "        sites_dictionary = {}\n",
    "        \n",
    "        for user in tqdm_notebook(files):\n",
    "            for site in user.site.values:\n",
    "                if site in sites_dictionary:\n",
    "               \n",
    "                    sites_dictionary[site] += 1                \n",
    "                \n",
    "                else:\n",
    "                    sites_dictionary[site] = 1\n",
    "              \n",
    "        d =  sorted(sites_dictionary.items(), key=lambda item: item[1],reverse = True)    \n",
    "        sites_dict_sorted = {}\n",
    "    \n",
    "        for ii,pair in enumerate(d):\n",
    "        \n",
    "            sites_dict_sorted[pair[0]] = [ii+1,pair[1]]  #заполнение словаря ID и частотами из отсортированного d\n",
    "            \n",
    "    \n",
    "    # Заполнение таблицы пользовательских сессий ID просмотренных пользователем сайтов\n",
    "    Session_list = []  \n",
    "    \n",
    "    for ind,user in enumerate(tqdm_notebook(files)): \n",
    "        session = np.zeros(len(feature_names))\n",
    "        session[-1] = user_IDs[ind] #заполнение столбца ID пользователей\n",
    "         \n",
    "        sites = user.site.values\n",
    "        user.timestamps = pd.to_datetime(user.timestamp)#Столбец даты и времени начала просмотра сайта\n",
    "        weekdays = user.timestamps.apply(lambda x:x.weekday()).values\n",
    "        hours = user.timestamps.apply(lambda x:x.time().hour).values\n",
    "        seconds = user.timestamps.apply(lambda x:x.timestamp()).values\n",
    "        \n",
    "               \n",
    "        session[-2] = weekdays[0] # день недели просмотра сайта\n",
    "        session[-3] = hours[0] # час просмотра сайта\n",
    "        \n",
    "        ufile_length = sites.shape[0]\n",
    "        site_position = 0 #начальная позиция в сессии\n",
    "        pos_in_ufile = 0 #начальная позиция в файле\n",
    "        while pos_in_ufile < ufile_length: \n",
    "            session[site_position]=sites_dict_sorted[sites[pos_in_ufile]][0]#получение ID сайта из словаря по названию\n",
    "            if site_position > 0:\n",
    "                session[site_position + session_length-1] = seconds[pos_in_ufile] -\\\n",
    "                                                            seconds[pos_in_ufile-1] #время просмотра сайта\n",
    "            pos_in_ufile += 1 #шаг по файлу\n",
    "            site_position += 1 #шаг по сессии\n",
    "        \n",
    "            if site_position == session_length: #если достигнут конец сессии\n",
    "                session[-4] = np.unique(session[:session_length]).shape[0] #Количество уникальных сайтов в сессии\n",
    "                session[-5] = seconds[pos_in_ufile-1] -\\\n",
    "                                seconds[pos_in_ufile-session_length]          #длительность сессии   \n",
    "                Session_list.append(session) #готовая сессия добавляется в список\n",
    "                \n",
    "                session = np.zeros(len(feature_names))#инициализируется новая сессия\n",
    "                session[-1] = user_IDs[ind]\n",
    "                site_position = 0\n",
    "                pos_in_ufile -= session_length - window_size\n",
    "                if (pos_in_ufile != ufile_length):#если не достигнут конец файла\n",
    "                    session[-2] = weekdays[pos_in_ufile]\n",
    "                    session[-3] = hours[pos_in_ufile]\n",
    "                \n",
    "            \n",
    "            \n",
    "            if (pos_in_ufile == ufile_length) & (site_position != 0):#если файл закончился раньше сессии\n",
    "                    \n",
    "                session[-4] = np.unique(session[:site_position]).shape[0] \n",
    "                session[-5] = seconds[pos_in_ufile-1]-\\\n",
    "                                    seconds[pos_in_ufile-site_position]                  \n",
    "                Session_list.append(session)\n",
    "                \n",
    "                if site_position > window_size:#если каретка вышла за пределы окна (можно начать новую сессию)\n",
    "                    session = np.zeros(len(feature_names))\n",
    "                    session[-1] = user_IDs[ind]               \n",
    "                    pos_in_ufile -= site_position - window_size\n",
    "                    site_position = 0 \n",
    "                    session[-2] = weekdays[pos_in_ufile]\n",
    "                    session[-3] = hours[pos_in_ufile]\n",
    "    \n",
    "    \n",
    "    Sessions = pd.DataFrame(Session_list,columns=feature_names, dtype = int)\n",
    "        \n",
    "       \n",
    "    return Sessions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name_dict = {128: 'Mary-Kate', 39: 'Ashley', 207: 'Lindsey', 127: 'Naomi', 237: 'Avril',\n",
    "               33: 'Bob', 50: 'Bill', 31: 'John', 100: 'Dick', 241: 'Ed'}\n",
    "train_data_10users['target'] = train_data_10users['target'].map(id_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dic = {'Mary-Kate': 'pink', 'Ashley': 'darkviolet', 'Lindsey':'blueviolet', \n",
    "             'Naomi': 'hotpink', 'Avril': 'orchid', \n",
    "             'Bob': 'firebrick', 'Bill': 'gold', 'John': 'forestgreen', \n",
    "             'Dick': 'slategrey', 'Ed':'brown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_10users['session_timespan'],color='darkviolet',range = (0,200))\n",
    "\n",
    "plt.xlabel('Длина сессии в секундах,с')\n",
    "plt.ylabel('Количество сессий')\n",
    "plt.title('Гистограмма распределения длины сессии')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_10users['#unique_sites'],color='aqua')  \n",
    "plt.xlabel('Число уникальных сайтов в сессии')\n",
    "plt.ylabel('Количество сессий')\n",
    "plt.title('Гистограмма распределения числа уникальных сайтов в сессии')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "for idx,(user,sub_df) in enumerate(train_data_10users.groupby('target')):\n",
    "    ax = axes[idx // 4,idx % 4]\n",
    "    ax.hist(sub_df['#unique_sites'],color=color_dic[user])  \n",
    "    ax.set(xlabel='Число уникальных сайтов в сессии', ylabel='Количество сессий')\n",
    "    ax.legend([user])\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train_data_10users.groupby('target')): \n",
    "    ax = axes[idx // 4,idx % 4]\n",
    "    ax.hist(sub_df.start_hour,color = color_dic[user])\n",
    "    ax.set(xlabel = 'Час начала сессии',ylabel = 'Количество сессий')\n",
    "    ax.legend([user])\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_10users['day_of_week'], color = 'sienna',bins=train_data_10users['day_of_week'].unique().shape[0])\n",
    "plt.xlabel('дни недели')\n",
    "plt.ylabel('количество сессий')\n",
    "plt.title('Гистограмма распредления дня недели начала сессии')\n",
    "plt.xticks(np.linspace(0,6,7),['Пн', 'Вт', 'Ср', 'Чт', 'Пт', 'Сб', 'Вс'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train_data_10users.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df.day_of_week,color = color_dic[user],bins = sub_df.day_of_week.unique().shape[0])\n",
    "    ax.set(xlabel='дни недели',ylabel='количество сессий')\n",
    "    ax.set_xticks(range(7))\n",
    "    ax.set_xticklabels(['Пн', 'Вт', 'Ср', 'Чт', 'Пт', 'Сб', 'Вс'] )\n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame(site_freq_10users).T\n",
    "df_freq.columns = ['id','freq']\n",
    "df_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_freqs = list(df_freq[df_freq.id <= 10].freq)\n",
    "top10_sites = list(df_freq[df_freq.id <= 10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top10_sites)\n",
    "print(top10_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(top10_sites,top10_freqs)\n",
    "plt.ylabel('Частота посещения')\n",
    "\n",
    "plt.title('Частоты посещения топ-10 сайтов')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(path_to_csv_files, site_freq_path, feature_names,\n",
    "                                    session_length=10, window_size=10):\n",
    "    ''' ВАШ КОД ЗДЕСЬ '''\n",
    "    \n",
    "    \n",
    "    #Загрузка файлов пользователей\n",
    "    files_paths = list(glob(os.path.join(path_to_csv_files,'user*.csv')))\n",
    "    files = []\n",
    "    user_IDs = []\n",
    "    for path in tqdm_notebook(files_paths):\n",
    "        f_name = os.path.split(path)[1]\n",
    "        ID = int(re.findall(\"[0-9]+\",f_name)[0])\n",
    "        user_IDs.append(ID)\n",
    "        files.append(pd.read_csv(path))\n",
    "    user_IDs = np.array(user_IDs)\n",
    "            \n",
    "    #Получение словаря сайтов с ID и частотами\n",
    "    if (session_length==10) and (window_size==10):#если так, то подгружаем уже готовый\n",
    "        with open(site_freq_path,'rb') as f:\n",
    "            sites_dict_sorted = pickle.load(f)\n",
    "        f.close()\n",
    "    else:        #иначе создаем из файлов пользователй\n",
    "        sites_dictionary = {}\n",
    "        \n",
    "        for user in tqdm_notebook(files):\n",
    "            for site in user.site.values:\n",
    "                if site in sites_dictionary:\n",
    "               \n",
    "                    sites_dictionary[site] += 1                \n",
    "                \n",
    "                else:\n",
    "                    sites_dictionary[site] = 1\n",
    "              \n",
    "        d =  sorted(sites_dictionary.items(), key=lambda item: item[1],reverse = True)    \n",
    "        sites_dict_sorted = {}\n",
    "    \n",
    "        for ii,pair in enumerate(d):\n",
    "        \n",
    "            sites_dict_sorted[pair[0]] = [ii+1,pair[1]]  #заполнение словаря ID и частотами из отсортированного d\n",
    "            \n",
    "    \n",
    "    # Заполнение таблицы пользовательских сессий ID просмотренных пользователем сайтов\n",
    "    Session_list = []  \n",
    "    \n",
    "    for ind,user in enumerate(tqdm_notebook(files)): \n",
    "        session = np.zeros(len(feature_names))\n",
    "        session[-1] = user_IDs[ind] #заполнение столбца ID пользователей\n",
    "         \n",
    "        sites = user.site.values\n",
    "        user.timestamps = pd.to_datetime(user.timestamp)#Столбец даты и времени начала просмотра сайта\n",
    "        weekdays = user.timestamps.apply(lambda x:x.weekday()).values\n",
    "        hours = user.timestamps.apply(lambda x:x.time().hour).values\n",
    "        seconds = user.timestamps.apply(lambda x:x.timestamp()).values\n",
    "        \n",
    "               \n",
    "        session[-2] = weekdays[0] # день недели просмотра сайта\n",
    "        session[-3] = hours[0] # час просмотра сайта\n",
    "        \n",
    "        ufile_length = sites.shape[0]\n",
    "        site_position = 0 #начальная позиция в сессии\n",
    "        pos_in_ufile = 0 #начальная позиция в файле\n",
    "        while pos_in_ufile < ufile_length: \n",
    "            session[site_position]=sites_dict_sorted[sites[pos_in_ufile]][0]#получение ID сайта из словаря по названию\n",
    "            if site_position > 0:\n",
    "                session[site_position + session_length-1] = seconds[pos_in_ufile] -\\\n",
    "                                                            seconds[pos_in_ufile-1] #время просмотра сайта\n",
    "            \n",
    "            \n",
    "            \n",
    "            #ДОБАВЛЕННЫЕ ПРИЗНАКИ (дабавляются с конца списка признаков и после уже известных)\n",
    "            \n",
    "            site_id = int(session[site_position])\n",
    "            dict_len = len(sites_dict_sorted)\n",
    "            \n",
    "                \n",
    "            #Время просмотра одного из топ-30 сайтов словаря\n",
    "            if site_position > 0:\n",
    "                if site_id <= 30:\n",
    "                    \n",
    "                    session[-5 - 31 + site_id] += seconds[pos_in_ufile] -\\\n",
    "                                                            seconds[pos_in_ufile-1]   \n",
    "            \n",
    "            \n",
    "            # Индикаторы посещения топ-30 сайтов из словаря\n",
    "            if site_id <= 30:\n",
    "                if session[-5 - 61 + site_id] == 0:\n",
    "                    session[-5 - 61 + site_id] = 1 \n",
    "            \n",
    "            \n",
    "            \n",
    "            #ПРИЗНАКИ на основе предпочтений посещения сайтов (частоты)\n",
    "            \n",
    "            #Частоты посещения 10 не самых популярных, но более специфичных сайтов, чем лидеры\n",
    "            if (site_id <= dict_len//50) and (site_id > (dict_len//50)-10):\n",
    "                position10 = site_id - (dict_len//50-10) #позиция признака по порядку следования (1...10)\n",
    "                session[-5 - 71 + position10] += 1 #увеличивается частота посещения\n",
    "            \n",
    "            #Частоты посещения 10  самых популярных сайтов\n",
    "            if site_id <= 10:                \n",
    "                session[-5 - 81 + site_id] += 1 #увеличивается частота посещения       \n",
    "                     \n",
    "           \n",
    "            \n",
    "            #конец составления добавочных признаков\n",
    "                \n",
    "            \n",
    "            pos_in_ufile += 1 #шаг по файлу\n",
    "            site_position += 1 #шаг по сессии\n",
    "        \n",
    "            if site_position == session_length: #если достигнут конец сессии\n",
    "                session[-4] = np.unique(session[:session_length]).shape[0] #Количество уникальных сайтов в сессии\n",
    "                session[-5] = seconds[pos_in_ufile-1] -\\\n",
    "                                seconds[pos_in_ufile-session_length]     #длительность сессии        \n",
    "                Session_list.append(session) #готовая сессия добавляется в список\n",
    "                \n",
    "                session = np.zeros(len(feature_names))#инициализируется новая сессия\n",
    "                session[-1] = user_IDs[ind]\n",
    "                site_position = 0\n",
    "                pos_in_ufile -= session_length - window_size\n",
    "                if (pos_in_ufile != ufile_length):#если не достигнут конец файла\n",
    "                    session[-2] = weekdays[pos_in_ufile]\n",
    "                    session[-3] = hours[pos_in_ufile]\n",
    "                \n",
    "            \n",
    "            \n",
    "            if (pos_in_ufile == ufile_length) & (site_position != 0):#если файл закончился раньше сессии\n",
    "                    \n",
    "                session[-4] = np.unique(session[:site_position]).shape[0] \n",
    "                session[-5] = seconds[pos_in_ufile-1]-\\\n",
    "                                    seconds[pos_in_ufile-site_position]                  \n",
    "                Session_list.append(session)\n",
    "                \n",
    "                if site_position > window_size:#если каретка вышла за пределы окна (можно начать новую сессию)\n",
    "                    session = np.zeros(len(feature_names))\n",
    "                    session[-1] = user_IDs[ind]               \n",
    "                    pos_in_ufile -= site_position - window_size\n",
    "                    site_position = 0 \n",
    "                    session[-2] = weekdays[pos_in_ufile]\n",
    "                    session[-3] = hours[pos_in_ufile]\n",
    "    \n",
    "    \n",
    "    Sessions = pd.DataFrame(Session_list,columns=feature_names, dtype = int)\n",
    "        \n",
    "       \n",
    "    return Sessions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10users_many_fe['target'] = train10users_many_fe['target'].map(id_name_dict) #назначаем имена пользователям\n",
    "#далее будем использовать еще словарь цветов пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = ['time_diff' + str(i) for i in range(1,5)]\n",
    "sns.pairplot(train10users_many_fe[sub1+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 =  ['freq__top10_' + str(i) for i in range(1,5)]\n",
    "sns.pairplot(train10users_many_fe[sub2+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 =  ['freq__top10_' + str(i) for i in range(1,5)]\n",
    "sns.pairplot(train10users_many_fe[sub3+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4 = ['ind_top'+str(i) for i in range(1,5)]\n",
    "sns.pairplot(train10users_many_fe[sub4+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub5 = ['time_top'+str(i) for i in range(1,5)]\n",
    "sns.pairplot(train10users_many_fe[sub5+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub6 = ['session_timespan', '#unique_sites', 'start_hour','day_of_week']\n",
    "sns.pairplot(train10users_many_fe[sub6+['target']],hue='target',palette=color_dic, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "number = []\n",
    "users = []\n",
    "for (user, sub_df) in  train10users_many_fe.groupby('target'): \n",
    "    number.append(sub_df.shape[0]) \n",
    "    users.append(user)\n",
    "    \n",
    "trace = go.Bar(\n",
    "    x = users,\n",
    "    y = number\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Количество сессий в зависимости от пользователя',\n",
    "    \n",
    ")\n",
    "\n",
    "fig = go.Figure(data = [trace], layout = layout)\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='#e9e9e9')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='#e9e9e9')\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "\n",
    "fig.data[0].marker.line.color = \"black\"\n",
    "fig.data[0].marker.color = \"LightSeaGreen\"\n",
    "fig.data[0].hoverlabel.bgcolor = \"white\"\n",
    "\n",
    "\n",
    "\n",
    "iplot(fig)\n",
    "print(sum(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def f(num):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.barplot(users,train10users_many_fe.groupby('target')['time_diff'+str(num)].mean(),palette = color_dic)\n",
    "    plt.ylabel('Среднее время за сессию')\n",
    "    plt.title(f'Средние значения для времени просмотра {num} сайта из сессии')    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, num=widgets.IntSlider(min=1, max=9, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(num):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.barplot(users,train10users_many_fe.groupby('target')['freq__top10_'+str(num)].mean(),palette = color_dic)\n",
    "    plt.ylabel('Средняя частота за сессию')\n",
    "    plt.title(f'Средняя частота просмотра сайта {num} за сессию из топ 10 сайтов')    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f2, num=widgets.IntSlider(min=1, max=10, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(num):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.barplot(users,train10users_many_fe.groupby('target')['freq__mid10_'+str(num)].mean(),palette = color_dic)\n",
    "    plt.ylabel('Средняя частота за сессию')\n",
    "    plt.title(f'Средняя частота просмотра сайта {num} за сессию из 10 менее популярных сайтов')    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f3, num=widgets.IntSlider(min=1, max=10, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(num):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.barplot(users,train10users_many_fe.groupby('target')['time_top'+str(num)].mean(),palette = color_dic)\n",
    "    plt.ylabel('Среднее время за сессию')\n",
    "    plt.title(f'Средние значения для времени просмотра сайта {num} из топ 30 сайтов')    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f4, num=widgets.IntSlider(min=1, max=30, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train10users_many_fe.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df[sub_df.time_diff1<1000][sub_df.time_diff1>100].time_diff1,color = color_dic[user])\n",
    "    ax.set(xlabel='time_diff1',ylabel='количество сессий')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train10users_many_fe.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df[sub_df.freq__top10_1>0].freq__top10_1,color = color_dic[user])\n",
    "    ax.set(xlabel='freq__top10_1',ylabel='количество сессий')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train10users_many_fe.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df[sub_df.freq__mid10_1>0].freq__mid10_1,color = color_dic[user])\n",
    "    ax.set(xlabel='freq__mid10_1',ylabel='количество сессий')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train10users_many_fe.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df.ind_top1,color = color_dic[user])\n",
    "    ax.set(xlabel='ind_top1',ylabel='количество сессий')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "\n",
    "\n",
    "for idx, (user, sub_df) in  enumerate(train10users_many_fe.groupby('target')): \n",
    "    ax = axes[idx//4,idx%4]    \n",
    "    ax.hist(sub_df[sub_df.time_top1>0].time_top1,color = color_dic[user])\n",
    "    ax.set(xlabel='time_top1',ylabel='количество сессий')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.legend([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_new_dim = TSNE(n_components=2,random_state=42).fit_transform(new_features_10users.values)\n",
    "X_new_dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_dim_feat = pd.DataFrame(X_new_dim,columns=['feat1','feat2'])\n",
    "df_new_dim =pd.concat([df_new_dim_feat,train10users_many_fe.target],axis=1) \n",
    "df_new_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "   \n",
    "for (user,sub_df) in df_new_dim.groupby('target'):\n",
    "    plt.scatter(sub_df.feat1,sub_df.feat2,color = color_dic[user],alpha=0.5,label=user)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funk(num):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sub_df = df_new_dim[df_new_dim.target == users[num-1]]\n",
    "    plt.scatter(sub_df.feat1,sub_df.feat2,color = color_dic[users[num-1]],alpha=0.3) \n",
    "    plt.legend([users[num-1]])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(funk, num=widgets.IntSlider(min=1, max=10, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_dim_cos = TSNE(n_components=2,random_state=42,metric ='cosine').fit_transform(new_features_10users.values)\n",
    "X_new_dim_cos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_dim_feat_cos = pd.DataFrame(X_new_dim_cos,columns=['feat1','feat2'])\n",
    "df_new_dim_cos =pd.concat([df_new_dim_feat_cos,train10users_many_fe.target],axis=1) \n",
    "df_new_dim_cos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "   \n",
    "for (user,sub_df) in df_new_dim_cos.groupby('target'):\n",
    "    plt.scatter(sub_df.feat1,sub_df.feat2,color = color_dic[user],alpha=0.5,label=user)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funk2(num):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sub_df = df_new_dim_cos[df_new_dim_cos.target == users[num-1]]\n",
    "    plt.scatter(sub_df.feat1,sub_df.feat2,color = color_dic[users[num-1]],alpha=0.3) \n",
    "    plt.legend([users[num-1]])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(funk2, num=widgets.IntSlider(min=1, max=10, step=1, value=1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curves(param_values, grid_cv_results_):\n",
    "    train_mu, train_std = grid_cv_results_['mean_train_score'], grid_cv_results_['std_train_score']\n",
    "    valid_mu, valid_std = grid_cv_results_['mean_test_score'], grid_cv_results_['std_test_score']\n",
    "    train_line = plt.plot(param_values, train_mu, '-', label='train', color='green')\n",
    "    valid_line = plt.plot(param_values, valid_mu, '-', label='test', color='red')\n",
    "    plt.fill_between(param_values, train_mu - train_std, train_mu + train_std, edgecolor='none',\n",
    "                     facecolor=train_line[0].get_color(), alpha=0.2)\n",
    "    plt.fill_between(param_values, valid_mu - valid_std, valid_mu + valid_std, edgecolor='none',\n",
    "                     facecolor=valid_line[0].get_color(), alpha=0.2)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_curves(svm_params1['C'], svm_grid_searcher1.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(val_train, val_test, train_sizes, \n",
    "                        xlabel='Training Set Size', ylabel='score'):\n",
    "    def plot_with_err(x, data, **kwargs):\n",
    "        mu, std = data.mean(1), data.std(1)\n",
    "        lines = plt.plot(x, mu, '-', **kwargs)\n",
    "        plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                         facecolor=lines[0].get_color(), alpha=0.2)\n",
    "    plot_with_err(train_sizes, val_train, label='train')\n",
    "    plot_with_err(train_sizes, val_test, label='valid')\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(val_train, val_test, n_train, \n",
    "                    xlabel='train_size', ylabel='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_train_set_time(path_to_csv_files, \n",
    "                                    session_time=5):\n",
    "       \n",
    "    #Загрузка файлов пользователей\n",
    "    files_paths = list(glob(os.path.join(path_to_csv_files,'user*.csv')))\n",
    "    files = []\n",
    "    user_IDs = []\n",
    "    for path in tqdm_notebook(files_paths):\n",
    "        f_name = os.path.split(path)[1]\n",
    "        ID = int(re.findall(\"[0-9]+\",f_name)[0])\n",
    "        user_IDs.append(ID)\n",
    "        files.append(pd.read_csv(path))\n",
    "    user_IDs = np.array(user_IDs)\n",
    "            \n",
    "    #Получение словаря сайтов с ID и частотами\n",
    "   \n",
    "    sites_dictionary = {}\n",
    "    \n",
    "    for user in tqdm_notebook(files):\n",
    "        for site in user.site.values:\n",
    "            if site in sites_dictionary:\n",
    "               \n",
    "                sites_dictionary[site] += 1                \n",
    "                \n",
    "            else:\n",
    "                sites_dictionary[site] = 1\n",
    "              \n",
    "    d =  sorted(sites_dictionary.items(), key=lambda item: item[1],reverse = True)    \n",
    "    sites_dict_sorted = {}\n",
    "    \n",
    "    for ii,pair in enumerate(d):\n",
    "        \n",
    "        sites_dict_sorted[pair[0]] = [ii+1,pair[1]]  #заполнение словаря ID и частотами из отсортированного d\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    # Заполнение таблицы пользовательских сессий ID просмотренных пользователем сайтов при заданной длительности сессии\n",
    "    Session_list = [] \n",
    "    for ind,user in enumerate(tqdm_notebook(files)):\n",
    "        session = []#инициализация сессии\n",
    "        session.append(user_IDs[ind])#нулевой столбец для ID пользователей\n",
    "        \n",
    "        sites = user.site.values#стоблец сайтов пользователя\n",
    "        \n",
    "        \n",
    "        \n",
    "        user.timestamps = pd.to_datetime(user.timestamp)#Столбец даты и времени начала просмотра сайта\n",
    "        \n",
    "        seconds = user.timestamps.apply(lambda x:x.timestamp()).values\n",
    "        \n",
    "        \n",
    "       \n",
    "        ufile_length = sites.shape[0] #длина файла пользователя\n",
    "        site_position = 0 #позиция в сессии\n",
    "        pos_in_ufile = 0 #позиция в файле\n",
    "        \n",
    "        duration_time_session = 0\n",
    "        duration_site_on_last_session = 0\n",
    "        while pos_in_ufile < ufile_length:\n",
    "            if pos_in_ufile > 0:\n",
    "                #Время на сайте в текущей сессии\n",
    "                duration_time_site =(seconds[pos_in_ufile] - seconds[pos_in_ufile-1])/60. - duration_site_on_last_session \n",
    "                duration_site_on_last_session  = 0 #новая сессии, величина пока не нужна\n",
    "                \n",
    "                duration_time_session += duration_time_site #увеличиваем время сессии\n",
    "                \n",
    "                \n",
    "            if duration_time_session  > session_time:  #время сессии превышено            \n",
    "                Session_list.append(session) #Сессия закончена и помещена в список\n",
    "                #print('new')\n",
    "                session = [] #инициализация новой сессии\n",
    "                session.append(user_IDs[ind]) #нулевой столбец для ID пользователей\n",
    "                site_to_start = sites[pos_in_ufile-1]#последний сайт прошлой переходит на новую сессию\n",
    "                duration_time_site_over_edge = duration_time_session - session_time#время выхода за пределы\n",
    "                duration_site_on_last_session  = (seconds[pos_in_ufile] - seconds[pos_in_ufile-1])/60.\\\n",
    "                                                    - duration_time_site_over_edge #время, проведенное на прошлой сессии\n",
    "                \n",
    "                duration_time_session = 0\n",
    "                session.append(sites_dict_sorted[site_to_start][0])\n",
    "                \n",
    "            elif duration_time_session == session_time:\n",
    "                Session_list.append(session)\n",
    "                #print('new')\n",
    "                session = [] #инициализация новой сессии\n",
    "                session.append(user_IDs[ind]) #нулевой столбец для ID пользователей\n",
    "                site_to_start = sites[pos_in_ufile]\n",
    "                duration_time_session = 0\n",
    "                session.append(sites_dict_sorted[site_to_start][0])\n",
    "                pos_in_ufile += 1                \n",
    "            else:\n",
    "                site_to_add =  sites[pos_in_ufile] #сайт для добавления в сессию\n",
    "                session.append(sites_dict_sorted[site_to_add][0])#добавление\n",
    "                pos_in_ufile += 1 #переходим на следующую позицию и на след шаге будет проверять время на сайте\n",
    "            \n",
    "        if len(session) > 0: #обрабатываем последнюю сессию для данного пользователя\n",
    "            \n",
    "                \n",
    "            Session_list.append(session) #Добавляем последнюю сессию в список  \n",
    "                \n",
    "   \n",
    "    \n",
    "    Sessions = pd.DataFrame(Session_list).fillna(0).values\n",
    "      \n",
    "    \n",
    "    #Отделений ID пользователей от таблицы\n",
    "    X, y = Sessions[:,1:],Sessions[:,0]\n",
    "    \n",
    "    #Получение разреженной матрицы частот встречаемости сайтов в каждой сессии      \n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for s in tqdm_notebook(X):\n",
    "        for ID in s:\n",
    "            index = vocabulary.setdefault(ID, ID)\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    X_sparse = csr_matrix((data, indices, indptr), dtype=int)[:,1:] \n",
    "    \n",
    "    \n",
    "    return X_sparse, y, Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assessment(estimator, path_to_X_pickle, path_to_y_pickle, cv, random_state=17, test_size=0.3):\n",
    "    \n",
    "    #Загрузка\n",
    "    with open(path_to_X_pickle,'rb') as X_pickle:\n",
    "        X_sparse = pickle.load(X_pickle)\n",
    "    \n",
    "    with open(path_to_y_pickle,'rb') as y_pickle:\n",
    "        y = pickle.load(y_pickle)\n",
    "    \n",
    "    #Разбиение на обучающую и валидационную выборки    \n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(X_sparse,y,\\\n",
    "                                                       random_state=random_state,test_size=test_size,stratify=y)\n",
    "    \n",
    "    #Средняя точность на кроссвалидации для обучающей выборки    \n",
    "    mean_cv_accuracy = cross_val_score(estimator,X_train,y_train, cv=cv).mean()\n",
    "    \n",
    "    #Точность для валидационной выборки\n",
    "    estimator.fit(X_train,y_train)\n",
    "    val_accuracy = accuracy_score(y_valid,estimator.predict(X_valid))\n",
    "    \n",
    "    return mean_cv_accuracy,val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'capstone_user_identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sparse = spar(train_test_df_sites.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = train_test_sparse[:253561]\n",
    "X_test_sparse = train_test_sparse[253561:]\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{X_train_sparse.shape[0]} {X_train_sparse.shape[1]} {X_test_sparse.shape[0]} {X_test_sparse.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log',random_state=17,n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "logit_valid_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_AUC = roc_auc_score(y_valid,logit_valid_pred_proba[:,1])\n",
    "print(round(ROC_AUC,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparse,y)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_sparse)\n",
    "logit_test_pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.00001,0.0001,0.00015,0.0002,0.001,0.1,1],\n",
    "             'l1_ratio':[0,0.1,0.15,0.5],\n",
    "             'class_weight': ['balanced',None,{1:1000,0:0.001}]}\n",
    "clf = SGDClassifier(loss='log',n_jobs=-1,random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "\n",
    "sgd_grid_searcher = GridSearchCV(clf,param_grid,cv=skf,n_jobs=-1,return_train_score=True,scoring='roc_auc')\n",
    "sgd_grid_searcher.fit(X_train, y_train)\n",
    "sgd_grid_searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_grid_searcher.best_score_,sgd_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_AUC1 = roc_auc_score(y_valid,sgd_grid_searcher.best_estimator_.predict_proba(X_valid)[:,1])\n",
    "print(round(ROC_AUC1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_check = {'learning_rate': ['optimal','invscaling','adaptive']}\n",
    "             \n",
    "clf_check = SGDClassifier(loss='log',random_state=91,learning_rate = 'adaptive',eta0=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_grid_searcher_ch = GridSearchCV(clf_check,param_grid_check,cv=skf,n_jobs=-1,return_train_score=True,scoring='roc_auc')\n",
    "sgd_grid_searcher_ch.fit(X_train, y_train)\n",
    "sgd_grid_searcher_ch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(3),sgd_grid_searcher_ch.cv_results_['mean_test_score'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd_grid2 = SGDClassifier(loss='log',alpha=0.00005,class_weight='balanced',random_state=17,\\\n",
    "                          learning_rate = 'adaptive',eta0=0.01)\n",
    "sgd_grid2.fit(X_train,y_train) \n",
    "ROC_AUC2 = roc_auc_score(y_valid,sgd_grid2.predict_proba(X_valid)[:,1])\n",
    "print(round(ROC_AUC2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_AUC2 = roc_auc_score(y_valid,sgd_grid2.predict_proba(X_valid)[:,1])\n",
    "print(round(ROC_AUC2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "train_test_sparse_tfidf = transformer.fit_transform(train_test_sparse)\n",
    "train_test_sparse_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_top1 = np.zeros(sessions_sites.shape[0],dtype=int)\n",
    "ind_top2 = np.zeros(sessions_sites.shape[0],dtype=int)\n",
    "unique_sites = np.zeros(sessions.shape[0],dtype=int)\n",
    "for i,session in enumerate(sessions_sites):\n",
    "    unique_sites[i] = np.unique(session).shape[0]\n",
    "    if 1 in session:\n",
    "        ind_top1[i] = 1\n",
    "    if 2 in session:\n",
    "        ind_top2[i] = 1\n",
    "        \n",
    "sum(ind_top1),sum(ind_top2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isWeekend = pd.to_datetime(train_test_df['time1']).apply(lambda x: 1 if x.weekday() in [5,6] else 0).values\n",
    "isWeekend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = pd.to_datetime(train_test_df['time1']).apply(lambda x: x.month).values\n",
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = pd.to_datetime(train_test_df['time1']).apply(lambda x: x.year).values\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe1 = np.vstack((ind_top1.T,ind_top2.T,isWeekend,month,year))[:3].T\n",
    "added_fe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat1 = pd.get_dummies(np.vstack((ind_top1.T,ind_top2.T,isWeekend,month,year))[-1].T)\n",
    "added_fe_cat1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat2 = pd.get_dummies(month)\n",
    "added_fe_cat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hour =pd.to_datetime(train_test_df['time1']).apply(lambda x:x.time().hour).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat3 = pd.get_dummies(start_hour)\n",
    "added_fe_cat3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat4 = pd.get_dummies(unique_sites)\n",
    "added_fe_cat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = pd.to_datetime(train_test_df['time1']).apply(lambda x:x.weekday()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat5 = pd.get_dummies(day_of_week)\n",
    "added_fe_cat5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeOfDay = pd.to_datetime(train_test_df['time1']).apply(lambda x:x.time().hour).apply(lambda x: 0 if x in range(7) else\\\n",
    "                                                                                      (1 if x in range(7,12) else \\\n",
    "                                                                                      (2 if x in range(12,18)\\\n",
    "                                                                                      else 3))).values\n",
    "TimeOfDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat6 = pd.get_dummies(TimeOfDay)\n",
    "added_fe_cat6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = pd.to_datetime(train_test_df['time1']).apply(lambda x: x.month).apply(lambda x: 1 if x in [12,1,2] else\\\n",
    "                                                                                      (2 if x in [3,4,5] else \\\n",
    "                                                                                      (3 if x in [6,7,8]\\\n",
    "                                                                                      else 4))).values\n",
    "season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat7 = pd.get_dummies(season)\n",
    "added_fe_cat7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_counts_top5 = train_test_sparse[:,:5].todense()\n",
    "sum(added_fe_counts_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,session in enumerate(sessions_sites):\n",
    "    num_sites[i] = sum(session>0)\n",
    "    for ID in range(1,11):\n",
    "        if ID in session:\n",
    "            num_sites_top10[i] += 1           \n",
    "(np.unique(num_sites),np.bincount(num_sites)) , (np.unique(num_sites_top10),np.bincount(num_sites_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat8 = pd.get_dummies(num_sites)\n",
    "added_fe_cat8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat9 = pd.get_dummies(num_sites_top10)\n",
    "added_fe_cat9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_df[['time%d' % i for i in range(1, 11)]]\n",
    "\n",
    "tdf = train_test_df_time.fillna(0)\n",
    "for i,col in tqdm_notebook(enumerate(tdf.columns)):\n",
    "    tdf[f'sec{i+1}'] = pd.to_datetime(tdf[col]).apply(lambda x: x.timestamp())\n",
    "\n",
    "session_times = tdf[['sec%d' % i for i in range(1, 11)]]\n",
    "session_times.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_sites_10sec = np.zeros(session_times.shape[0],dtype=int)\n",
    "\n",
    "for i,session in tqdm_notebook(enumerate(session_times.values)):     \n",
    "    num = 0\n",
    "    j = 0\n",
    "    duration = 0\n",
    "    while (duration < 10) and (duration >= 0):\n",
    "        num += 1\n",
    "        j += 1\n",
    "        if (j == session.shape[0]):\n",
    "            break\n",
    "        duration += session[j] - session[j-1]\n",
    "    \n",
    "    num_sites_10sec[i] = num   \n",
    "\n",
    "num_sites_10sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_fe_cat10 = pd.get_dummies(num_sites_10sec)\n",
    "added_fe_cat10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, col in enumerate(session_times.columns):\n",
    "    if ind > 0:\n",
    "        session_times[f'dur{ind}'] =  session_times[col] - session_times[session_times.columns[ind-1]]\n",
    "session_times.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = session_times[[f'dur{i}' for i in range(1,10)]]\n",
    "durations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_array = np.array(durations,dtype = float)\n",
    "dur_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,session in enumerate(dur_array):\n",
    "    for j,dur in enumerate(session):\n",
    "        if dur < 0:\n",
    "            if j==0:\n",
    "                dur_array[i,j]=30*60\n",
    "            else:\n",
    "                dur_array[i,j]=30*60 - sum(session[:j])\n",
    "pd.DataFrame(dur_array).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(dur_array)\n",
    "time_fee_to_add= scaler.transform(dur_array)\n",
    "time_fee_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse7 = hstack((train_test_sparse_tfidf[:253561],added_fe1[:253561],\\\n",
    "                          added_fe_cat1.values[:253561],added_fe_cat2.values[:253561],\\\n",
    "                          added_fe_cat3.values[:253561],added_fe_cat4.values[:253561],\\\n",
    "                          added_fe_cat5.values[:253561],added_fe_cat6.values[:253561],\\\n",
    "                          added_fe_cat7.values[:253561],added_fe_counts_top5[:253561],\\\n",
    "                          added_fe_cat8.values[:253561],added_fe_cat9.values[:253561],added_fe_cat10.values[:253561],\\\n",
    "                          time_fee_to_add[:253561]))\n",
    "X_test_sparse7 = hstack((train_test_sparse_tfidf[253561:],added_fe1[253561:],\\\n",
    "                         added_fe_cat1.values[253561:],added_fe_cat2.values[253561:],\\\n",
    "                         added_fe_cat3.values[253561:],added_fe_cat4.values[253561:],\\\n",
    "                         added_fe_cat5.values[253561:],added_fe_cat6.values[253561:],\\\n",
    "                         added_fe_cat7.values[253561:],added_fe_counts_top5[253561:],\\\n",
    "                         added_fe_cat8.values[253561:],added_fe_cat9.values[253561:],added_fe_cat10.values[253561:],\\\n",
    "                         time_fee_to_add[253561:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd_logit_grid55 = SGDClassifier(loss='log',alpha=0.00005,class_weight='balanced',random_state=17,\\\n",
    "                          learning_rate = 'adaptive',eta0=0.01)\n",
    "sgd_logit_grid55.fit(X_train_sparse7,y)\n",
    "logit_test_pred_proba55 = sgd_logit_grid55.predict_proba(X_test_sparse7)\n",
    "logit_test_pred_proba55[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_pick = xgb.XGBClassifier(random_state=17,n_estimators=10)\n",
    "param_grid_boost = {'learning_rate':[0.01,0.1,0.2,0.5],\n",
    "                    'max_depth':[3,5,7], \n",
    "                    'min_child_weight':[1,3,5]\n",
    "                   }\n",
    "gb_grid_searcher = GridSearchCV(estimator_pick,param_grid_boost,cv=skf,n_jobs=-1,return_train_score=True,scoring='roc_auc')\n",
    "gb_grid_searcher.fit(X_train3, y_train3)\n",
    "gb_grid_searcher.best_score_,gb_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100]\n",
    "rocs = []\n",
    "for n in n_estimators:\n",
    "    gb_clf = xgb.XGBClassifier(learning_rate=0.5,max_depth=7, min_child_weight=1, random_state=17,n_estimators=n)\n",
    "    gb_clf.fit(X_train4,y_train4)\n",
    "    ROC_AUC_test_fe7 = roc_auc_score(y_valid4,gb_clf.predict_proba(X_valid4)[:,1])\n",
    "    rocs.append(ROC_AUC_test_fe7)\n",
    "    print(round(ROC_AUC_test_fe7,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gb_clf55 = xgb.XGBClassifier(learning_rate=0.5,max_depth=7, min_child_weight=1, random_state=17,n_estimators=100)\n",
    "gb_clf55.fit(X_train_sparse7,y)\n",
    "gb_test_pred_proba55 = gb_clf55.predict_proba(X_test_sparse7)\n",
    "gb_test_pred_proba55[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf1 = RandomForestClassifier(n_estimators = 10,  random_state=17)\n",
    "params_forest = {\"class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "                 \"criterion\": [\"gini\", \"entropy\"],\n",
    "                 \"min_samples_split\":[7,5,3]\n",
    "                }\n",
    "forest_grid_searcher = GridSearchCV(forest_clf1,param_grid = params_forest,cv=skf,n_jobs=-1,scoring='roc_auc')\n",
    "forest_grid_searcher.fit(X_train4, y_train4)\n",
    "forest_grid_searcher.best_score_,forest_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,120]\n",
    "rocs = []\n",
    "for n in n_estimators:\n",
    "    forest_clf2 = RandomForestClassifier(n_estimators = n, class_weight='balanced',criterion='gini',min_samples_split= 7, random_state=17)\n",
    "    forest_clf2.fit(X_train4,y_train4)\n",
    "    ROC_AUC_test_fe9 = roc_auc_score(y_valid4,forest_clf2.predict_proba(X_valid4)[:,1])\n",
    "    rocs.append(ROC_AUC_test_fe9)\n",
    "    print(round(ROC_AUC_test_fe9,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forest_clf55 = RandomForestClassifier(n_estimators = 150, class_weight='balanced',criterion='gini',min_samples_split= 7, random_state=17)\n",
    "forest_clf55.fit(X_train_sparse7,y)\n",
    "forest_test_pred_proba55 = forest_clf55.predict_proba(X_test_sparse7)\n",
    "forest_test_pred_proba55[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_test_pred_proba555 = (logit_test_pred_proba55[:,1]+gb_test_pred_proba55[:,1] + forest_test_pred_proba55[:,1])/3\n",
    "blended_test_pred_proba555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(blended_test_pred_proba555,out_file='submission31_tfidf_fe_new_new_blend_l_b_f.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неделя 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_vw_add_fe(Xs,Xc,Xn, y=None, train=True, out_file='palpatin.vw'):\n",
    "    \n",
    "    Xs = np.array(Xs,dtype=int)\n",
    "    Xc = np.array(Xc)\n",
    "    Xn = np.array(Xn,dtype=float)\n",
    "    \n",
    "    if train:\n",
    "        with open(out_file, 'w') as vw_train_data:         \n",
    "            for yy,xs,xc,xn in zip(y,Xs,Xc,Xn): \n",
    "                obj = str(yy)+' |session '+ ' '.join(map(str,xs)) +\\\n",
    "                      ' |cat ' + ' '.join([f'Fc{i+1}={val}' for i,val in enumerate(xc)]) +\\\n",
    "                      ' |num ' + ' '.join([f'Fn{i+1}:{val}' for i,val in enumerate(xn)]) + '\\n'\n",
    "                vw_train_data.write(obj)\n",
    "    else:\n",
    "        with open(out_file, 'w') as vw_test_data:         \n",
    "            for xs,xc,xn in zip(Xs,Xc,Xn): \n",
    "                obj = '1 |session '+ ' '.join(map(str,xs)) +\\\n",
    "                      ' |cat ' + ' '.join([f'Fc{i+1}={val}' for i,val in enumerate(xc)]) +\\\n",
    "                      ' |num ' + ' '.join([f'Fn{i+1}:{val}' for i,val in enumerate(xn)]) + '\\n'\n",
    "                vw_test_data.write(obj)\n",
    "                \n",
    "    print('Good, Anakin, good!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "arrays_to_vw(train_df_part.fillna(0).values, y_train_part_for_vw,\\\n",
    "             out_file = os.path.join(PATH_TO_DATA2,'train_part.vw'))\n",
    "arrays_to_vw(valid_df.fillna(0).values, y_valid_for_vw,\\\n",
    "             out_file = os.path.join(PATH_TO_DATA2,'valid.vw')) \n",
    "arrays_to_vw(train_df_400[sites].fillna(0).values, y_for_vw,\\\n",
    "             out_file = os.path.join(PATH_TO_DATA2,'train.vw'))\n",
    "arrays_to_vw(test_df_400[sites].fillna(0).values,\\\n",
    "             train = False, out_file = os.path.join(PATH_TO_DATA2,'test.vw'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -4 $PATH_TO_DATA2/train_part.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_vw = os.path.join(PATH_TO_DATA2, 'train_part.vw')\n",
    "valid_vw = os.path.join(PATH_TO_DATA2, 'valid.vw')\n",
    "train_vw = os.path.join(PATH_TO_DATA2, 'train.vw')\n",
    "test_vw = os.path.join(PATH_TO_DATA2, 'test.vw')\n",
    "model = os.path.join(PATH_TO_DATA2, 'vw_model.vw')\n",
    "valid_pred = os.path.join(PATH_TO_DATA2,'vw_valid_pred.csv')\n",
    "pred = os.path.join(PATH_TO_DATA2, 'vw_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (loss, passes, bits) in itertools.product(['squared','logistic','hinge'],[2,3,5],[18,26,30]):\n",
    "    \n",
    "    !vw --oaa 400 --passes $passes -c -k -b $bits --loss_function $loss --random_seed 17 -d $train_part_vw -f $model --quiet\n",
    "    \n",
    "    !vw -i $model -t -d $valid_vw -p $valid_pred --quiet\n",
    "    \n",
    "    \n",
    "    print(f'Params: {loss} {passes} {bits}, Accuracy: {accuracy_score(y_valid_for_vw,pd.read_csv(valid_pred,header=None))}')\n",
    "    print('---------------')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for reg in [1e-10,1e-15,1e-20]:\n",
    "    \n",
    "    !vw --oaa 400 --passes 5 -c -k -b 26 --loss_function logistic --l1 $reg --random_seed 17\\\n",
    "    -d $train_part_vw -f $model --quiet\n",
    "    \n",
    "    !vw -i $model -t -d $valid_vw -p $valid_pred --quiet\n",
    "    \n",
    "    \n",
    "    print(f'Param reg L1: {reg} , Accuracy: {accuracy_score(y_valid_for_vw,pd.read_csv(valid_pred,header=None))}')\n",
    "    print('---------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for reg in [5e-8,1e-9,5e-9,1e-10,2e-10,3e-10,4e-10,5e-10,1e-11]:\n",
    "    \n",
    "    !vw --oaa 400 --passes 2 -c -k -b 18 --loss_function logistic --l2 $reg --random_seed 17\\\n",
    "    -d $train_part_vw -f $model --quiet\n",
    "    \n",
    "    !vw -i $model -t -d $valid_vw -p $valid_pred --quiet\n",
    "    \n",
    "    \n",
    "    print(f'Param reg L2: {reg} , Accuracy: {accuracy_score(y_valid_for_vw,pd.read_csv(valid_pred,header=None))}')\n",
    "    print('---------------') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw --help\n",
    "!cp --help\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA2, 'X_sparse_150users_s10_w10.pkl'), 'rb') as X_sparse_150users_pkl:\n",
    "     X_sparse_150users = pickle.load(X_sparse_150users_pkl)\n",
    "with open(os.path.join(PATH_TO_DATA2, 'y_150users_s10_w10.pkl'), 'rb') as y_150users_pkl:\n",
    "    y_150users = pickle.load(y_150users_pkl)\n",
    "\n",
    "class_distr = np.bincount(y_train_150.astype('int'))\n",
    "\n",
    "\n",
    "train10users_many_fe[['time_top'+str(i) for i in range(1,31)]].head()\n",
    "\n",
    "for num_users in [10,150]:\n",
    "    for window_size,session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]):\n",
    "        if (window_size<=session_length) and ((session_length,window_size) != (10,10)):\n",
    "            print(num_users,session_length,window_size)\n",
    "            \n",
    "assert X_sparse_10users.shape[1] == len(site_freq_10users)-1\n",
    "\n",
    "\n",
    "\n",
    "df['education'].value_counts().plot.barh();\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    label_encoder.transform(df['education'].replace('high.school', 'high_school'))\n",
    "except Exception as e:\n",
    "    print('Error:', e)\n",
    "    \n",
    "    \n",
    "    \n",
    "def logistic_regression_accuracy_on(dataframe, labels):\n",
    "    features = dataframe.as_matrix()\n",
    "    train_features, test_features, train_labels, test_labels = \\\n",
    "        train_test_split(features, labels)\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    logit.fit(train_features, train_labels)\n",
    "    return classification_report(test_labels, logit.predict(test_features))\n",
    "\n",
    "print(logistic_regression_accuracy_on(df[categorical_columns], labels))\n",
    "\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_categorical_columns = pd.DataFrame(onehot_encoder.fit_transform(df[categorical_columns]))\n",
    "encoded_categorical_columns.head()\n",
    "\n",
    "\n",
    "%%bash\n",
    "ls\n",
    "\n",
    "!echo '1 1.0 |Subject WHAT car is this |Organization University of Maryland:0.5 College Park' | vw\n",
    "\n",
    "\n",
    "\n",
    "def to_vw_format(document, label=None):\n",
    "    return str(label or '') + ' |text ' + ' '.join(re.findall('\\w{3,}', document.lower())) + '\\n'\n",
    "\n",
    "to_vw_format(text, 1 if target == 'rec.autos' else -1)\n",
    "\n",
    "\n",
    "\n",
    "with open('./20news_test_predictions.txt') as pred_file:\n",
    "    test_prediction = [float(label) \n",
    "                             for label in pred_file.readlines()]\n",
    "\n",
    "auc = roc_auc_score(test_labels, test_prediction)\n",
    "roc_curve_gr = roc_curve(test_labels, test_prediction)\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(roc_curve_gr[0], roc_curve_gr[1]);\n",
    "    plt.plot([0,1], [0,1])\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('test AUC = %f' % (auc)); plt.axis([-0.05,1.05,-0.05,1.05]);\n",
    "    \n",
    "    \n",
    "    \n",
    "M = confusion_matrix(test_labels_mult, test_prediction_mult)\n",
    "for i in np.where(M[0,:] > 0)[0][1:]:\n",
    "    print(newsgroups['target_names'][i], M[0,i], )\n",
    "    \n",
    "    \n",
    "reviews_test = load_files(os.path.join(path_to_movies, 'test'))\n",
    "text_test, y_test = reviews_test.data, reviews_train.target\n",
    "print(\"Number of documents in test data: %d\" % len(text_test))\n",
    "print(np.bincount(y_test))\n",
    "\n",
    "\n",
    "\n",
    "!cat --help\n",
    "\n",
    "\n",
    "!du -hs $PATH_TO_DATA/stackoverflow_10mln_*.vw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
